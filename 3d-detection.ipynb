{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"f2214caa-3e43-4ec6-9088-8a57e33316c7","_cell_guid":"95be8d7d-8d0e-4a07-9257-c8de15481d0d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport pandas as pd\nimport torchvision.transforms as transforms","metadata":{"_uuid":"b6f8ceba-047b-42c9-ae31-03e381b411ae","_cell_guid":"e926d579-5307-4b96-b226-bcafa6f4a974","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"9d76b20c-bc90-4917-a71b-ca0ae6c1cfee","_cell_guid":"f4682ac9-d711-4e24-8cc5-8f9f6368aa6b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/early-detection-of-3d-printing-issues/train.csv')","metadata":{"_uuid":"c185e474-d7a2-4035-a8e8-5f6860032ca2","_cell_guid":"3576eb89-766e-470d-8788-ef79952a1b1c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport cv2\n\nclass CDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None,):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = cv2.imread(img_path,)\n        label = self.img_labels.iloc[idx, -1]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"_uuid":"1f218689-5360-4f92-8c12-86694df74ab7","_cell_guid":"0e3933cd-e532-4833-9117-41b9009e87b3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"f1ad6d52-089e-4463-88c9-227bb07f1c8f","_cell_guid":"a9d368a0-031c-4272-abc9-6ca2d94cdeaa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        transforms.Resize(IMAGE_SIZE)\n    ]\n)","metadata":{"_uuid":"59e2c820-0c2d-4f2b-b0e2-fc61ca3d5be3","_cell_guid":"c96600b7-50a9-454a-ae0e-42c1c9f6ec54","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CDataset(\n    '/kaggle/input/early-detection-of-3d-printing-issues/train.csv',\n    '/kaggle/input/early-detection-of-3d-printing-issues/images/' ,\n    transform\n)","metadata":{"_uuid":"1907b187-3deb-45cf-8734-5e0fd22e8f83","_cell_guid":"dc0fe1e8-37d9-4b7b-b665-d9e152593718","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.sampler import SubsetRandomSampler\n\nBATCH_SIZE = 4\nIMAGE_SIZE = (256,256)\nvalidation_split=.2\nshuffle_dataset = True\nrandom_seed=12\n\n\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n                                                sampler=valid_sampler)","metadata":{"_uuid":"4854e984-660f-47ca-bd17-2deee6838529","_cell_guid":"7cd6c996-d66d-432c-a8dc-ed7fc630b2d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))","metadata":{"_uuid":"3d33671f-78a2-459f-95a5-f1c47088b146","_cell_guid":"ea60fb8f-a5cc-40cc-9386-251f58603ba6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net_VGG(torch.nn.Module):\n    \"\"\"\n    This class is uses VGG netowrk as pretrained network\n    \"\"\"\n\n    def __init__(self, num_classes=5):\n        \"\"\"\n        This function initializes the model\n        \"\"\"\n        super(Net_VGG, self).__init__()\n        self.model = torch.hub.load(\"pytorch/vision:v0.6.0\", \"vgg16\", pretrained=True)\n        self.model.classifier[6] = torch.nn.Linear(4096, num_classes)\n        self.backbone = self.model\n\n    def forward(self, x):\n        \"\"\"\n        This function defines the forward pass of the model\n        \"\"\"\n        x = self.backbone(x)\n        x = F.normalize(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\n\nnet = Net_VGG()\nnet.to(device)","metadata":{"_uuid":"da5a7179-df22-4b65-bf87-08216e7be2a3","_cell_guid":"dd6d0250-d206-45ca-98f7-9aeac4e8f303","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","metadata":{"_uuid":"0109c818-49f9-4c13-9d55-23867c6d1934","_cell_guid":"37a3cd11-ece6-4554-b6f8-399141a311ee","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')","metadata":{"_uuid":"3447a081-edf6-44a9-a739-3a604c8ef180","_cell_guid":"68d9d79f-a924-48e4-b80d-dd65baf328c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"6d809d1f-1035-4246-8c3e-2e7c2d272449","_cell_guid":"4737d1de-889f-4e49-9d2d-19870a3a535f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}